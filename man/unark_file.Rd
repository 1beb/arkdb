% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/unark.R
\name{unark_file}
\alias{unark_file}
\title{Unarchive a single tsv file into an existing database}
\usage{
unark_file(filename, db_con, lines = 10000L)
}
\arguments{
\item{filename}{a *.tsv.bz2 file to uncompress}

\item{db_con}{a database src (\code{src_dbi} object from \code{dplyr})}

\item{lines}{number of lines to read in a chunk.}
}
\value{
the database connection (\code{src_dbi}, invisibly)
}
\description{
Unarchive a single tsv file into an existing database
}
\details{
\code{unark_file} will read in a file in chunks and
write them into a database.  This is essential for processing
large compressed tables which may be too large to read into
memory before writing into a database.  In general, increasing
the \code{lines} parameter will result in a faster total transfer
but require more free memory for working with these larger chunks.
}
\examples{
\donttest{

## set up example files and database
tsv <- tempfile("flights", fileext=".tsv.bz2")
sqlite <- tempfile("nycflights", fileext=".sql")
readr::write_tsv(nycflights13::flights, tsv)
db <- src_sqlite(sqlite, create = TRUE)

## and here we go:
db_con <- unark_file(tsv, db)

## display tables in database:
db_con

unlink(tsv)
unlink(sql)
}

}
